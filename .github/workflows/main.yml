# 名前を決めよう
name: data_download_test

# ワークフローをいつ実行するかを決めます
# 手動実行のみ
on: [ workflow_dispatch ]
  # # マスターブランチに対してのみトリガー 一旦テストでブランチ他に作りたいからパス
  # push:
  #   branches: [ master ]
  # pull_request:
  #   branches: [ master ]
  
  # # 実行時間
  # schedule:
  #   - cron: '0 23 * * *'


# 実行内容
jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.7, 3.8]

    steps:
      # リポジトリをチェックアウト
      - uses: actions/checkout@v2
        with:
          ref: use_github_actions
    
      # AWSの認証情報を取得する
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          # aws-access-key-id: aaaaa # 読んでるかチェック
          # aws-secret-access-key: bbbbb # 読んでるかチェック
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-1

      # setup-python@v2っていうのを使う
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      # Pandasをインストール (一気にインストールできるらしいけどね)
      - name: Install Pandas
        run: pip install pandas==1.2.0
      # pydata_google_authをインストール(多分認証通らないだろうが)
      - name: Install pydata_google_auth
        run: pip install pydata_google_auth==1.1.0
      # gopogle.cloud bigqueryをインストール
      - name: Install google-cloud-bigquery
        run: pip install google-cloud-bigquery==2.20.0

      
      # # GCPの認証情報を取得する
      # - name: Configure GCP credentials
      #   uses: google-github-actions/get-gke-credentials@main
      #   with:
      #     cluster_name: my-first-cluster-1

      # Pythonファイルを実行
      - name: Run data download
        run: python data_auto_download_for_actions.py

      # Pandasで加工したcsvファイルをS3へアップロード
      - name: Upload file to S3
        run: aws s3 cp ./chart.csv s3://yuzu-charts/chart.csv --quiet